behaviors: # К какому компоненту обращаемся.
  RollerBall: # имя компонента.
    trainer_type: ppo # тип тренера. ppo означает proximal policy optomozation. Есть еще SAC.
    # гиперпараметры, общие для и ppo, и sac.
    hyperparameters:  
      batch_size: 10 # кол-во опыта(предпринятых действий, вознаграждений и тд), передаваемых на каждой итерации градиентного спуска.
      buffer_size: 100 # размер буффера.
      learning_rate: 3.0e-4 # коэффициент скорости обучения, позволяет управлять величиной коррекции весов на каждой итерации.
      learning_rate_schedule: linear # режим обучения.
    # Специфические гиперпараметры конкретно для ppo.
      beta: 5.0e-4 # коеффициент энтропии.
      epsilon: 0.2 # ограничение.
      lambd: 0.99 # параметр GAE (автокодера графов). В диапазоне от 0.9 до 1.
      num_epoch: 3 # количество эпох, на сколько я понял, полных циклов обработки обучающих данных.
    # настройка нейронной сети
    network_settings: 
      normalize: false # следует ли нормализировать преимущество.
      hidden_units: 128 # Количество units в скрытых слоях нейронной сети. Чем сложнее взаимодействие, тем больше это кол-во.
      num_layers: 2 # Количество скрытый слоев в нейронной сети.
    reward_signals: # определение сигналов вознаграждения.
      extrinsic: # внешний сигнал вознаграждения. В данном случае вознаграждение от среды игнорируется.
        gamma: 0.99 # чем больше значение, тембольше агента волнует возможное вознаграждение в отдаленном будущем.
        strength: 1.0 # на что можно умножить необработанное вознаграждение. От 0 до 1.
    max_steps: 500000 # кол-во шагов моделирования, умноженных на частоту кадров, выполняемых в процессе обучения. Чем сложнее задача, тем больше должно быть число.
    time_horizon: 64 # как много шагов опыта нужно собрать прежде, чем добавлять его в буффер опыта. Должно таким, чтобы охватывать все важное поведение агента в последовательности его действий.
    summary_freq: 10000 # как часто в шаге сохранять статистику тренировок.